{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Plan\n",
    "- Parsed the data from a file containing URL addresses.\n",
    "- Extracted the data from the file and converted it into JSON format.\n",
    "- Annotated the data using Label Studio.\n",
    "- Downloaded the annotated data.\n",
    "- Split the annotated data into training and testing sets.\n",
    "- Finally, trained models using the training data."
   ],
   "id": "8e400ab280f48b3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Function for splitting annotation data",
   "id": "a60039b9cff9eec"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spacy\n",
    "from spacy.tokens import DocBin\n",
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "def convert_data_to_spacy_format(input_file, train_output_file, dev_output_file, split_ratio=0.8):\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    train_doc_bin = DocBin()\n",
    "    dev_doc_bin = DocBin()\n",
    "\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    random.shuffle(data)\n",
    "\n",
    "    split_index = int(len(data) * split_ratio)\n",
    "\n",
    "    for idx, item in enumerate(data):\n",
    "        text = item['data']['text']\n",
    "        entities = []\n",
    "\n",
    "        for annotation in item['annotations']:\n",
    "            for result in annotation['result']:\n",
    "                if 'value' in result:\n",
    "                    entity = result['value']\n",
    "                    start = entity['start']\n",
    "                    end = entity['end']\n",
    "                    label = entity['labels'][0]\n",
    "                    entities.append((start, end, label))\n",
    "\n",
    "        doc = nlp.make_doc(text)  \n",
    "        ents = []\n",
    "\n",
    "        for start, end, label in entities:\n",
    "            span = doc.char_span(start, end, label=label)\n",
    "            if span is not None:\n",
    "                ents.append(span)\n",
    "\n",
    "        doc.ents = ents\n",
    "\n",
    "        if idx < split_index:\n",
    "            train_doc_bin.add(doc)\n",
    "        else:\n",
    "            dev_doc_bin.add(doc)\n",
    "\n",
    "    train_doc_bin.to_disk(train_output_file)\n",
    "    dev_doc_bin.to_disk(dev_output_file)\n",
    "\n",
    "\n",
    "convert_data_to_spacy_format(\"data/furniture_data.json\", \"train.spacy\", \"dev.spacy\", split_ratio=0.8)"
   ],
   "id": "6b60a1f1e5ea091c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "spacy model for cpu using vectors = \"en_core_web_md\". This model use for site because it's a great combination of high performance, precision and small model size.",
   "id": "b7f905b3c1e5b469"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-05T09:38:38.860728Z",
     "start_time": "2024-10-05T09:30:53.608688Z"
    }
   },
   "cell_type": "code",
   "source": "! python -m spacy train config.cfg --output ./output --paths.train ./train.spacy --paths.dev ./dev.spacy -g 0",
   "id": "20822202876b9a69",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4m[i] Saving to output directory: output\u001B[0m\n",
      "\u001B[38;5;4m[i] Using GPU: 0\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "\u001B[38;5;2m[+] Initialized pipeline\u001B[0m\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "\u001B[38;5;4m[i] Pipeline: ['tok2vec', 'ner']\u001B[0m\n",
      "\u001B[38;5;4m[i] Initial learn rate: 0.001\u001B[0m\n",
      "E    #       LOSS TOK2VEC  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  ------------  --------  ------  ------  ------  ------\n",
      "  0       0          0.00     55.83    0.00    0.00    0.00    0.00\n",
      "  0     200         51.27   1817.63   16.57   31.87   11.20    0.17\n",
      "  1     400         13.36    967.30   45.63   61.44   36.29    0.46\n",
      "  2     600         18.74    794.85   68.73   68.73   68.73    0.69\n",
      "  3     800         25.11    739.95   71.31   75.98   67.18    0.71\n",
      "  4    1000         36.70    685.57   72.07   80.48   65.25    0.72\n",
      "  6    1200         38.41    601.47   74.33   73.76   74.90    0.74\n",
      "  8    1400         65.36    586.20   78.03   83.33   73.36    0.78\n",
      " 10    1600         64.48    447.28   73.87   83.82   66.02    0.74\n",
      " 13    1800         94.72    450.44   74.90   77.37   72.59    0.75\n",
      " 17    2000         89.25    403.44   75.51   80.09   71.43    0.76\n",
      " 21    2200        101.66    412.48   71.83   81.07   64.48    0.72\n",
      " 26    2400        124.23    418.06   76.34   78.69   74.13    0.76\n",
      " 32    2600        139.96    378.60   74.13   78.45   70.27    0.74\n",
      " 37    2800        967.69    394.17   76.92   83.33   71.43    0.77\n",
      " 43    3000        183.81    332.12   74.85   78.15   71.81    0.75\n",
      "\u001B[38;5;2m[+] Saved pipeline to output directory\u001B[0m\n",
      "output\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "spacy model-transformer \"roberta-base\". This model too big for site.",
   "id": "bcc34846705f3c9e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T17:40:11.240781Z",
     "start_time": "2024-10-03T16:59:56.606591Z"
    }
   },
   "cell_type": "code",
   "source": "! python -m spacy train base_config_2.cfg --output ./output_2 --paths.train ./train.spacy --paths.dev ./dev.spacy -g 0",
   "id": "a349c2ab608ad6cb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4m[i] Saving to output directory: output_1\u001B[0m\n",
      "\u001B[38;5;4m[i] Using GPU: 0\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "\u001B[38;5;2m[+] Initialized pipeline\u001B[0m\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "\u001B[38;5;4m[i] Pipeline: ['transformer', 'ner']\u001B[0m\n",
      "\u001B[38;5;4m[i] Initial learn rate: 0.0\u001B[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0          97.49     94.96    1.72    0.89   29.73    0.02\n",
      "  2     200       17255.22  22057.62   62.38   52.51   76.83    0.62\n",
      "  5     400        1194.21   2098.01   76.37   74.81   77.99    0.76\n",
      "  8     600         636.04   1091.26   73.10   78.07   68.73    0.73\n",
      " 10     800         451.60    692.18   76.06   76.06   76.06    0.76\n",
      " 13    1000         311.35    500.91   79.22   79.69   78.76    0.79\n",
      " 16    1200         215.97    361.57   78.14   82.13   74.52    0.78\n",
      " 18    1400         190.80    322.11   79.19   83.05   75.68    0.79\n",
      " 21    1600         153.94    260.92   78.34   76.47   80.31    0.78\n",
      " 24    1800         163.63    263.02   79.44   83.12   76.06    0.79\n",
      " 26    2000         129.74    204.85   77.57   74.04   81.47    0.78\n",
      " 29    2200         139.18    244.92   79.68   81.45   77.99    0.80\n",
      " 32    2400         157.79    243.85   79.22   79.69   78.76    0.79\n",
      " 34    2600         118.27    184.27   79.69   79.84   79.54    0.80\n",
      " 37    2800          98.63    189.63   80.38   79.32   81.47    0.80\n",
      " 40    3000         101.02    182.58   78.66   80.57   76.83    0.79\n",
      " 42    3200          97.04    171.24   77.14   79.51   74.90    0.77\n",
      " 45    3400          76.60    147.01   80.40   82.52   78.38    0.80\n",
      " 48    3600          90.37    165.10   77.20   80.08   74.52    0.77\n",
      " 50    3800         102.50    180.59   77.99   77.99   77.99    0.78\n",
      " 53    4000          72.64    119.34   80.31   80.31   80.31    0.80\n",
      " 56    4200          82.91    148.49   77.90   73.38   83.01    0.78\n",
      " 58    4400          74.80    125.95   79.09   77.90   80.31    0.79\n",
      " 61    4600          83.15    140.57   76.41   77.17   75.68    0.76\n",
      " 63    4800          68.21    127.15   79.84   80.16   79.54    0.80\n",
      " 66    5000          78.31    138.63   79.15   79.15   79.15    0.79\n",
      "\u001B[38;5;2m[+] Saved pipeline to output directory\u001B[0m\n",
      "output_1\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\shims\\pytorch.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "spacy model-transformer \"roberta-base\". This model too big for site.",
   "id": "79dd56b53eac802"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-03T21:26:57.153807Z",
     "start_time": "2024-10-03T20:55:19.134699Z"
    }
   },
   "cell_type": "code",
   "source": "! python -m spacy train base_config_3.cfg --output ./output_3 --paths.train ./train.spacy --paths.dev ./dev.spacy -g 0",
   "id": "1c39ca1219b84aa4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m[+] Created output directory: output_2\u001B[0m\n",
      "\u001B[38;5;4m[i] Saving to output directory: output_2\u001B[0m\n",
      "\u001B[38;5;4m[i] Using GPU: 0\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "\u001B[38;5;2m[+] Initialized pipeline\u001B[0m\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "\u001B[38;5;4m[i] Pipeline: ['transformer', 'ner']\u001B[0m\n",
      "\u001B[38;5;4m[i] Initial learn rate: 0.0\u001B[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0          34.00     60.32    1.74    0.90   28.96    0.02\n",
      "  2     200        9058.36  20002.66   23.80   34.56   18.15    0.24\n",
      "  5     400        1564.59   3047.22   68.67   77.29   61.78    0.69\n",
      "  8     600         769.34   1657.35   76.48   81.30   72.20    0.76\n",
      " 10     800         571.34   1142.62   75.49   77.33   73.75    0.75\n",
      " 13    1000         451.80    869.98   77.97   76.10   79.92    0.78\n",
      " 16    1200         351.81    668.13   75.64   77.64   73.75    0.76\n",
      " 18    1400         282.50    530.48   78.14   78.29   77.99    0.78\n",
      " 21    1600         217.37    418.40   79.78   76.79   83.01    0.80\n",
      " 24    1800         186.68    378.79   79.76   81.20   78.38    0.80\n",
      " 26    2000         150.93    290.04   79.46   79.01   79.92    0.79\n",
      " 29    2200         131.75    266.90   78.28   76.00   80.69    0.78\n",
      " 32    2400         126.11    232.41   78.07   75.27   81.08    0.78\n",
      " 34    2600          98.81    198.28   78.79   77.32   80.31    0.79\n",
      " 37    2800          92.00    189.97   78.85   78.54   79.15    0.79\n",
      " 40    3000          93.71    198.37   79.45   81.38   77.61    0.79\n",
      " 42    3200          88.84    181.98   79.62   77.86   81.47    0.80\n",
      "\u001B[38;5;2m[+] Saved pipeline to output directory\u001B[0m\n",
      "output_2\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\shims\\pytorch.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (561 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "spacy model-transformer \"distilbert-base-uncased\". This model has an optimal ratio of size and accuracy, but requires additional libraries to run that are too large to deploy on the site.",
   "id": "7facb853c523a8ce"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T21:35:37.774267Z",
     "start_time": "2024-10-04T21:14:34.956400Z"
    }
   },
   "cell_type": "code",
   "source": "! python -m spacy train base_config_4.cfg --output ./output_4 --paths.train ./train.spacy --paths.dev ./dev.spacy -g 0",
   "id": "adbe99c877cb8118",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;2m[+] Created output directory: output_3\u001B[0m\n",
      "\u001B[38;5;4m[i] Saving to output directory: output_3\u001B[0m\n",
      "\u001B[38;5;4m[i] Using GPU: 0\u001B[0m\n",
      "\u001B[1m\n",
      "=========================== Initializing pipeline ===========================\u001B[0m\n",
      "\u001B[38;5;2m[+] Initialized pipeline\u001B[0m\n",
      "\u001B[1m\n",
      "============================= Training pipeline =============================\u001B[0m\n",
      "\u001B[38;5;4m[i] Pipeline: ['transformer', 'ner']\u001B[0m\n",
      "\u001B[38;5;4m[i] Initial learn rate: 0.0\u001B[0m\n",
      "E    #       LOSS TRANS...  LOSS NER  ENTS_F  ENTS_P  ENTS_R  SCORE \n",
      "---  ------  -------------  --------  ------  ------  ------  ------\n",
      "  0       0          86.43    102.60    1.58    0.82   26.25    0.02\n",
      "  2     200       19681.46  22923.91   64.69   78.89   54.83    0.65\n",
      "  5     400        1123.88   1524.90   66.97   81.67   56.76    0.67\n",
      "  8     600         450.18    620.73   77.27   83.11   72.20    0.77\n",
      " 10     800         293.34    404.34   76.98   79.18   74.90    0.77\n",
      " 13    1000         242.80    328.66   79.47   78.28   80.69    0.79\n",
      " 16    1200         208.23    295.23   77.37   73.36   81.85    0.77\n",
      " 18    1400         184.00    284.76   79.01   78.11   79.92    0.79\n",
      " 21    1600         136.37    223.12   78.52   75.44   81.85    0.79\n",
      " 24    1800         146.83    232.77   77.82   78.43   77.22    0.78\n",
      " 26    2000         123.26    193.39   79.68   82.30   77.22    0.80\n",
      " 29    2200         152.95    237.70   79.16   78.41   79.92    0.79\n",
      " 32    2400         139.37    203.51   78.96   82.08   76.06    0.79\n",
      " 34    2600         125.88    200.88   80.39   81.67   79.15    0.80\n",
      " 37    2800         108.35    201.71   79.46   79.01   79.92    0.79\n",
      " 40    3000         118.26    194.07   79.31   78.71   79.92    0.79\n",
      " 42    3200         100.22    186.14   79.09   77.90   80.31    0.79\n",
      " 45    3400         136.42    194.27   79.53   81.12   77.99    0.80\n",
      " 48    3600         107.67    183.94   79.24   78.20   80.31    0.79\n",
      " 50    3800         106.37    191.91   78.29   75.36   81.47    0.78\n",
      " 53    4000         102.92    164.51   78.67   79.76   77.61    0.79\n",
      " 56    4200         114.65    197.59   79.22   79.69   78.76    0.79\n",
      "\u001B[38;5;2m[+] Saved pipeline to output directory\u001B[0m\n",
      "output_3\\model-last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\shims\\pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "C:\\Users\\vlads\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\thinc\\shims\\pytorch.py:128: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To improve the model's accuracy, we can utilize larger and more powerful transformer models, gather and use more data for training, as well as leverage web hosting services that provide greater capabilities and memory capacity.",
   "id": "86638fc7f15d9f14"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
